# Neural Network from Scratch in Python
This repository contains a simple implementation of neural networks from scratch using Python and NumPy. It includes various activation functions, layers, loss functions, optimizers, and utilities for training and evaluating neural networks.

## Features :tophat:
 1. Activation Functions: ReLU, Tanh, Sigmoid, Softmax
 2. Layers: Linear Layer
 3. Loss Functions: Categorical Cross Entropy, Binary Cross Entropy
 4. Optimizers: SGD, SGD with Momentum, AdaGrad, RMSProp, Adadelta, Adam
 5. Metrics: Accuracy
 6. Utilities: DataLoader for batching and shuffling data, Learning Rate Scheduler

## Contributing :open_hands:
Contributions are welcome!
>[!NOTE]
> If you'd like to contribute, please follow these steps:

- Fork the repository
- Create a new branch for your feature or bug fix
- Make your changes and commit them
- Push your changes to your forked repository
- Submit a pull request
